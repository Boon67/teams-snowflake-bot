# ğŸ“¡ Bot Framework Emulator Streaming

## Real-time Delta Streaming to Bot Framework Emulator

This feature enables real-time streaming of AI responses to the Bot Framework Emulator, so users can see the text appearing progressively as it's generated by Snowflake Cortex Agents.

## ğŸŒŠ How It Works

### **Server-side Flow:**
1. **Real-time SSE Processing**: Snowflake deltas are processed immediately as they arrive
2. **Delta Callback**: Each text delta triggers a callback to the bot
3. **Progressive Messaging**: Text is accumulated and sent in chunks to the emulator
4. **Final Formatting**: Complete response is formatted and sent at the end

### **User Experience:**
```
User: "who sold the most policies?"

Bot: ğŸ” Working on it...

Bot: ğŸ”„ Processing your request...     â–Š
     [Text appears progressively in the same message bubble]
     Based on the analysis of your insurance policies data, I can
     determine that Sarah Johnson sold the most policies with a total
     of 247 policies sold this year.

Bot: âœ… Processing complete!
     [Same message bubble now shows complete text]

Bot: [Final formatted response with tables and analysis]
```

## ğŸ¯ Key Features

### **1. Progressive Message Updates**
- **Same message bubble**: Text appears progressively in one message (no multiple bubbles)
- **Character-based updates**: Updates every 30+ characters or 300ms intervals
- **Typewriter effect**: Creates smooth "typing" animation with cursor indicator (â–Š)
- **Smart throttling**: Prevents overwhelming the Bot Framework with too many updates

### **2. Real-time Indicators**
```javascript
// Stream progress indicators
ğŸ”„ Processing your request...    // Initial message
ğŸ“ [text chunks]                // Progressive content
âœ… Processing complete!          // Stream finished
[Final formatted response]       // Complete result
```

### **3. Fallback Support**
- **Graceful degradation**: Falls back to regular responses if streaming fails
- **Error handling**: Continues working even if individual stream updates fail
- **Compatibility**: Works with both streaming and non-streaming endpoints

## ğŸ”§ Technical Implementation

### **Progressive Update System**
```javascript
const deltaCallback = async (delta) => {
    // Extract text content from each delta
    if (content.type === 'text' && content.text) {
        accumulatedText += content.text;
        
        // Update the same message progressively
        if (shouldUpdate) {
            if (!streamingStarted) {
                // Send initial message
                streamingActivity = await context.sendActivity(
                    MessageFactory.text(`ğŸ”„ **Processing...**\n\n${accumulatedText} â–Š`)
                );
                streamingStarted = true;
            } else {
                // Update existing message with new content
                await context.updateActivity({
                    type: 'message',
                    id: streamingActivity.id,
                    text: `ğŸ”„ **Processing...**\n\n${accumulatedText} â–Š`
                });
            }
        }
    }
};
```

### **Progressive Update Configuration**
```javascript
// Progressive update parameters
const THROTTLE_MS = 300;        // Max update frequency (300ms)
const MIN_CHARS = 30;           // Minimum characters before update
const CURSOR_SYMBOL = ' â–Š';     // Typing cursor indicator
```

### **Message Flow**
1. **Acknowledgment**: `ğŸ” Working on it...` (separate message)
2. **Progressive Message**: `ğŸ”„ Processing your request... â–Š` (starts empty, grows progressively)
3. **Content Updates**: Same message updates with new text as it arrives
4. **Stream Complete**: `âœ… Processing complete!` (replaces processing indicator)
5. **Final Response**: Properly formatted response with tables, SQL, etc. (separate message)

## ğŸ“Š Debug Output

When `DEBUG_DELTAS=true` is enabled, you'll see detailed streaming logs:

```
ğŸ“¡ Stream delta #1: "Based"
ğŸ“¡ Stream delta #2: " on"
ğŸ“¡ Stream delta #3: " the"
ğŸš€ Started streaming to emulator
ğŸ“¤ Sent streaming chunk: "Based on the analysis..."
ğŸ“¡ Stream delta #15: " policies"
ğŸ“¤ Sent streaming chunk: "I can determine that Sarah..."
âœ… Processing complete!
âœ… Sent final formatted response
```

## ğŸ® User Experience Examples

### **Simple Query**
```
User: "show me total policies"
Bot: ğŸ” Working on it...
Bot: ğŸ”„ Processing your request...
     Looking at your insurance
Bot: ğŸ“ policies data, I can see
Bot: ğŸ“ there are 1,247 total policies
Bot: âœ… Processing complete!
Bot: [Formatted table with policy breakdown]
```

### **Complex Analysis**
```
User: "analyze policy trends by agent"
Bot: ğŸ” Analyzing your request...
Bot: ğŸ”„ Processing your request...
     I'll analyze the policy trends
Bot: ğŸ“ across different agents to identify
Bot: ğŸ“ patterns in sales performance
Bot: ğŸ“ and growth opportunities
Bot: âœ… Processing complete!
Bot: [Detailed analysis with SQL, tables, and insights]
```

## ğŸ” Benefits

### **1. Immediate Feedback**
- Users see the AI is actively processing their request
- No more waiting in silence for complex queries
- Real-time progress indication

### **2. Better User Experience**
- **Engaging**: Users can follow the AI's thought process
- **Responsive**: Immediate visual feedback on query processing
- **Professional**: Smooth, modern streaming experience

### **3. Debug Visibility**
- **Real-time monitoring**: See exactly when deltas arrive
- **Performance tracking**: Monitor streaming speed and timing
- **Issue detection**: Quickly identify bottlenecks or failures

## âš™ï¸ Configuration

### **Enable Streaming**
```bash
# Start bot with streaming enabled
DEBUG_DELTAS=true npm run dev
```

### **Streaming Settings**
```javascript
// In bot.js - adjust these values for different streaming behavior
const THROTTLE_MS = 500;        // Faster = more frequent updates
const MIN_WORDS = 3;            // Lower = more frequent updates
```

### **Debug Levels**
```bash
DEBUG_DELTAS=true              # Full delta debugging + streaming
SHOW_DELTA_MESSAGES=true       # Basic delta info + streaming
# (no debug flags)             # Just streaming, no debug output
```

## ğŸš¨ Important Notes

### **Bot Framework Emulator Specific**
- **Works best with Bot Framework Emulator**: Real-time updates optimized for emulator
- **Teams compatibility**: May behave differently in production Teams environment
- **Fallback included**: Automatically falls back to regular messages if streaming fails

### **Performance Considerations**
- **Throttled updates**: Prevents overwhelming the Bot Framework with too many messages
- **Word-based chunking**: Sends meaningful text chunks, not character-by-character
- **Graceful degradation**: Continues working even if individual updates fail

### **Message Limits**
- **Bot Framework limits**: Respects message size and frequency limits
- **Chunking support**: Large responses are automatically chunked
- **Error handling**: Robust error handling for streaming failures

## ğŸ§ª Testing

### **Test Progressive Updates**
```bash
# Test progressive message updates (mock simulation)
npm run test-progressive-updates

# Test with real Bot Framework Emulator
DEBUG_DELTAS=true npm run dev
# Then send queries in Bot Framework Emulator
```

### **Test Commands**
```
# Simple queries (short streaming)
"show me total policies"
"who sold the most policies?"

# Complex queries (long streaming, more visible progressive updates)
"analyze policy trends by product type"
"generate a detailed report on agent performance"
```

### **Expected Behavior**
1. **Immediate acknowledgment**: `ğŸ” Working on it...` (separate message)
2. **Progressive typing**: `ğŸ”„ Processing your request... â–Š` (same message grows progressively)
3. **Text accumulation**: Text appears character by character with typing cursor
4. **Stream completion**: `âœ… Processing complete!` (cursor removed, message finalized)
5. **Final response**: Properly formatted complete answer (separate message)

---

ğŸ‰ **Real-time streaming is now active!** Users in Bot Framework Emulator will see AI responses appearing progressively as they're generated, creating a much more engaging and responsive experience!